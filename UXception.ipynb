{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skimage.transform import resize\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras import Model\n",
    "from keras.callbacks import  ModelCheckpoint\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\n",
    "from keras.layers import Conv2D, Concatenate, MaxPooling2D\n",
    "from keras.layers import UpSampling2D, Dropout, BatchNormalization\n",
    "from tqdm import tqdm_notebook\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "from keras.utils import conv_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.engine import InputSpec\n",
    "from keras import backend as K\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import ZeroPadding2D\n",
    "from keras.losses import binary_crossentropy\n",
    "import keras.callbacks as callbacks\n",
    "from keras.callbacks import Callback\n",
    "from keras.applications.xception import Xception\n",
    "from keras.layers import multiply\n",
    "\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.legacy import interfaces\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "from keras.engine.topology import Input\n",
    "from keras.engine.training import Model\n",
    "from keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\n",
    "from keras.layers.core import Activation, SpatialDropout2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import Input,Dropout,BatchNormalization,Activation,Add\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.core import Dense, Lambda\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply, Permute\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from skimage import measure\n",
    "from skimage.morphology import square, erosion, dilation, watershed\n",
    "import math\n",
    "\n",
    "import kaggle_util\n",
    "\n",
    "import cv2\n",
    "from albumentations import (\n",
    "    PadIfNeeded,\n",
    "    HorizontalFlip,\n",
    "    VerticalFlip,    \n",
    "    CenterCrop,    \n",
    "    Crop,\n",
    "    Compose,\n",
    "    Transpose,\n",
    "    RandomRotate90,\n",
    "    ElasticTransform,\n",
    "    GridDistortion, \n",
    "    OpticalDistortion,\n",
    "    RandomSizedCrop,\n",
    "    OneOf,\n",
    "    CLAHE,\n",
    "    RandomContrast,\n",
    "    RandomGamma,\n",
    "    RandomBrightness\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAR_SIZE = 384\n",
    "INPUT_SHAPE = (TAR_SIZE, TAR_SIZE)\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "PATH = './'\n",
    "TRAIN = '../input/train_v2/'\n",
    "TEST = '../input/test_v2/'\n",
    "SEGMENTATION = '../input/train_ship_segmentations_v2.csv.zip'\n",
    "exclude_list = ['6384c3e78.jpg','13703f040.jpg', '14715c06d.jpg',  '33e0ff2d5.jpg',\n",
    "                '4d4e09f2a.jpg', '877691df8.jpg', '8b909bb20.jpg', 'a8d99130e.jpg', \n",
    "                'ad55c3143.jpg', 'c8260c541.jpg', 'd6c7f17c7.jpg', 'dc3e7c901.jpg',\n",
    "                'e44dffe88.jpg', 'ef87bad36.jpg', 'f083256d8.jpg'] #corrupted images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = pd.read_csv(SEGMENTATION)\n",
    "folds = pd.read_csv('../input/folds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_train_set(folds, fold):\n",
    "#     folds = folds.loc[folds.has_ship > 0]\n",
    "#     train_fold = folds.loc[folds.fold != fold]\n",
    "#     val_fold = folds.loc[(folds.fold == fold) & (folds.holdout == 1)]\n",
    "    \n",
    "    \n",
    "#     return train_fold, val_fold\n",
    "\n",
    "\n",
    "# fold = 0\n",
    "# train_fold, val_fold = prepare_train_set(folds, fold)\n",
    "# VALID_IMG_COUNT = len(val_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "folds = folds.loc[folds.has_ship > 0]\n",
    "train_fold, val_fold = train_test_split(folds, test_size=0.05, random_state=42)\n",
    "VALID_IMG_COUNT = len(val_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len: 40428 val len 2128\n"
     ]
    }
   ],
   "source": [
    "print('train len:', len(train_fold), 'val len', len(val_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(img_id, df):\n",
    "    shape = (768,768)\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    masks = df.loc[df.ImageId == img_id]['EncodedPixels']\n",
    "    \n",
    "#     print(img_id, masks)\n",
    "    \n",
    "    if(type(masks.iloc[0]) == float): return img.reshape(shape)\n",
    "    for mask in masks:\n",
    "        s = mask.split()\n",
    "        for i in range(len(s)//2):\n",
    "            start = int(s[2*i]) - 1\n",
    "            length = int(s[2*i+1])\n",
    "            img[start:start+length] = 1\n",
    "    return img.reshape(shape).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "augments = [\n",
    "    HorizontalFlip(p=0.5),\n",
    "    VerticalFlip(p=0.5),\n",
    "    RandomRotate90(p=0.5),\n",
    "    Transpose(p=0.5),\n",
    "    RandomBrightness(limit = 0.1, p = 0.2),\n",
    "    RandomGamma(gamma_limit=(90, 110), p = 0.2),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_generator_sigmoid(val_df, masks, batch_size, input_shape, augments):\n",
    "    \n",
    "    while True:\n",
    "        inputs = []\n",
    "        outputs = []\n",
    "        step_id = 0\n",
    "        cnt = 0\n",
    "        for idx, row in val_df.iterrows():\n",
    "            img_id = row.ImageId\n",
    "            imgpath = os.path.join(TRAIN, '{0}'.format(img_id))\n",
    "            img = cv2.imread(imgpath, cv2.IMREAD_COLOR)\n",
    "            \n",
    "            msk = get_mask(img_id, masks)\n",
    "                \n",
    "            img = cv2.resize(img, input_shape)\n",
    "            msk = cv2.resize(msk, input_shape)\n",
    "            \n",
    "            for aug in augments:\n",
    "                augmented = aug(image=img, mask=msk)\n",
    "                img = augmented['image']\n",
    "                msk = augmented['mask']\n",
    "            \n",
    "            msk = msk[..., np.newaxis]\n",
    "            otp = msk\n",
    "            \n",
    "            inputs.append(img)\n",
    "            outputs.append(otp)\n",
    "         \n",
    "            if len(inputs) == batch_size:\n",
    "                step_id += 1\n",
    "                inputs = np.asarray(inputs)\n",
    "                outputs = np.asarray(outputs, dtype='float')\n",
    "#                 for img in inputs:\n",
    "#                     print(img.min(), img.max(), img.shape)\n",
    "                yield inputs, outputs\n",
    "                inputs = []\n",
    "                outputs = []\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_data_generator_sigmoid(val_df, masks, batch_size, validation_steps, input_shape):\n",
    "    while True:\n",
    "        inputs = []\n",
    "        outputs = []\n",
    "        step_id = 0\n",
    "        for idx, row in val_df.iterrows():\n",
    "            img_id = row.ImageId\n",
    "            imgpath = os.path.join(TRAIN, '{0}'.format(img_id))\n",
    "\n",
    "            img = cv2.imread(imgpath, cv2.IMREAD_COLOR)\n",
    "            msk = get_mask(img_id, masks)\n",
    "            \n",
    "            img = cv2.resize(img, input_shape)\n",
    "            msk = cv2.resize(msk, input_shape)\n",
    "            \n",
    "            msk = msk[..., np.newaxis]\n",
    "            otp = msk\n",
    "            \n",
    "            inputs.append(img)\n",
    "            outputs.append(otp)\n",
    "            \n",
    "            if len(inputs) == batch_size:\n",
    "                step_id += 1\n",
    "                inputs = np.asarray(inputs)\n",
    "                outputs = np.asarray(outputs, dtype='float')\n",
    "                yield inputs, outputs\n",
    "                inputs = []\n",
    "                outputs = []\n",
    "                if step_id == validation_steps:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_steps = math.ceil(len(val_fold) / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = train_data_generator_sigmoid(train_fold, masks, BATCH_SIZE, INPUT_SHAPE, augments)\n",
    "val_gen = val_data_generator_sigmoid(val_fold, masks, val_steps, val_steps, INPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x, val_y = next(val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/cpmpml/fast-iou-metric-in-numpy-and-tensorflow\n",
    "def get_iou_vector(A, B):\n",
    "    # Numpy version    \n",
    "    batch_size = A.shape[0]\n",
    "    metric = 0.0\n",
    "    for batch in range(batch_size):\n",
    "        t, p = A[batch], B[batch]\n",
    "        true = np.sum(t)\n",
    "        pred = np.sum(p)\n",
    "        \n",
    "        # deal with empty mask first\n",
    "        if true == 0:\n",
    "            metric += (pred == 0)\n",
    "            continue\n",
    "        \n",
    "        # non empty mask case.  Union is never empty \n",
    "        # hence it is safe to divide by its number of pixels\n",
    "        intersection = np.sum(t * p)\n",
    "        union = true + pred - intersection\n",
    "        iou = intersection / union\n",
    "        \n",
    "        # iou metrric is a stepwise approximation of the real iou over 0.5\n",
    "        iou = np.floor(max(0, (iou - 0.45)*20)) / 10\n",
    "        \n",
    "        metric += iou\n",
    "        \n",
    "    # teake the average over all images in batch\n",
    "    metric /= batch_size\n",
    "    return metric\n",
    "\n",
    "\n",
    "def my_iou_metric(label, pred):\n",
    "    # Tensorflow version\n",
    "    return tf.py_func(get_iou_vector, [label, pred > 0.5], tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "\n",
    "def bce_logdice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnapshotCallbackBuilder:\n",
    "    def __init__(self, nb_epochs, nb_snapshots, init_lr=0.1, fold = 0):\n",
    "        self.T = nb_epochs\n",
    "        self.M = nb_snapshots\n",
    "        self.alpha_zero = init_lr\n",
    "        self.fold = fold\n",
    "\n",
    "    def get_callbacks(self, model_prefix='Model'):\n",
    "\n",
    "        callback_list = [\n",
    "            callbacks.ModelCheckpoint(\"./models/XUnet{}_{}.model\".format(TAR_SIZE, self.fold),\n",
    "                                      monitor='val_my_iou_metric', \n",
    "                                   mode = 'max', save_best_only=True, verbose=1),\n",
    "            swa,\n",
    "            callbacks.LearningRateScheduler(schedule=self._cosine_anneal_schedule),\n",
    "            callbacks.TensorBoard(log_dir='./logs/XUnet{}_{}'.format(TAR_SIZE, self.fold)),\n",
    "        ]\n",
    "\n",
    "        return callback_list\n",
    "\n",
    "    def _cosine_anneal_schedule(self, t):\n",
    "        cos_inner = np.pi * (t % (self.T // self.M))  # t - 1 is used when t has 1-based indexing.\n",
    "        cos_inner /= self.T // self.M\n",
    "        cos_out = np.cos(cos_inner) + 1\n",
    "        return float(self.alpha_zero / 2 * cos_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n",
    "    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if activation == True:\n",
    "        x = LeakyReLU(alpha=0.1)(x)\n",
    "    return x\n",
    "\n",
    "def residual_block(blockInput, num_filters=16):\n",
    "    x = LeakyReLU(alpha=0.1)(blockInput)\n",
    "    x = BatchNormalization()(x)\n",
    "    blockInput = BatchNormalization()(blockInput)\n",
    "    x = convolution_block(x, num_filters, (3,3) )\n",
    "    x = convolution_block(x, num_filters, (3,3), activation=False)\n",
    "    x = Add()([x, blockInput])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/titu1994/keras-normalized-optimizers\n",
    "# Computes the L-2 norm of the gradient.\n",
    "def l2_norm(grad):\n",
    "    norm = K.sqrt(K.sum(K.square(grad))) + K.epsilon()\n",
    "    return norm\n",
    "\n",
    "class OptimizerWrapper(optimizers.Optimizer):\n",
    "\n",
    "    def __init__(self, optimizer):     \n",
    "        \n",
    "        self.optimizer = optimizers.get(optimizer)\n",
    "\n",
    "        # patch the `get_gradients` call\n",
    "        self._optimizer_get_gradients = self.optimizer.get_gradients\n",
    "\n",
    "    def get_gradients(self, loss, params):      \n",
    "        grads = self._optimizer_get_gradients(loss, params)\n",
    "        return grads\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        # monkey patch `get_gradients`\n",
    "        self.optimizer.get_gradients = self.get_gradients\n",
    "\n",
    "        # get the updates\n",
    "        self.optimizer.get_updates(loss, params)\n",
    "\n",
    "        # undo monkey patch\n",
    "        self.optimizer.get_gradients = self._optimizer_get_gradients\n",
    "\n",
    "        return self.updates\n",
    "\n",
    "    def set_weights(self, weights):       \n",
    "        self.optimizer.set_weights(weights)\n",
    "\n",
    "    def get_weights(self):        \n",
    "        return self.optimizer.get_weights()\n",
    "\n",
    "    def get_config(self):       \n",
    "        # properties of NormalizedOptimizer\n",
    "        config = {'optimizer_name': self.optimizer.__class__.__name__.lower()}\n",
    "\n",
    "        # optimizer config\n",
    "        optimizer_config = {'optimizer_config': self.optimizer.get_config()}\n",
    "        return dict(list(optimizer_config.items()) + list(config.items()))\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return self.optimizer.weights\n",
    "\n",
    "    @property\n",
    "    def updates(self):\n",
    "        return self.optimizer.updates\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @classmethod\n",
    "    def set_normalization_function(cls, name, func):\n",
    "        global _NORMS\n",
    "        _NORMS[name] = func\n",
    "\n",
    "    @classmethod\n",
    "    def get_normalization_functions(cls):        \n",
    "        global _NORMS\n",
    "        return sorted(list(_NORMS.keys()))\n",
    "\n",
    "\n",
    "class NormalizedOptimizer(OptimizerWrapper):\n",
    "\n",
    "    def __init__(self, optimizer, normalization='l2'):       \n",
    "        super(NormalizedOptimizer, self).__init__(optimizer)\n",
    "\n",
    "        if normalization not in _NORMS:\n",
    "            raise ValueError('`normalization` must be one of %s.\\n' \n",
    "                             'Provided was \"%s\".' % (str(sorted(list(_NORMS.keys()))), normalization))\n",
    "\n",
    "        self.normalization = normalization\n",
    "        self.normalization_fn = _NORMS[normalization]\n",
    "        self.lr = K.variable(1e-3, name='lr')\n",
    "\n",
    "    def get_gradients(self, loss, params):       \n",
    "        grads = super(NormalizedOptimizer, self).get_gradients(loss, params)\n",
    "        grads = [grad / self.normalization_fn(grad) for grad in grads]\n",
    "        return grads\n",
    "\n",
    "    def get_config(self):        \n",
    "        # properties of NormalizedOptimizer\n",
    "        config = {'normalization': self.normalization}\n",
    "\n",
    "        # optimizer config\n",
    "        base_config = super(NormalizedOptimizer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):       \n",
    "        optimizer_config = {'class_name': config['optimizer_name'],\n",
    "                            'config': config['optimizer_config']}\n",
    "\n",
    "        optimizer = optimizers.get(optimizer_config)\n",
    "        normalization = config['normalization']\n",
    "\n",
    "        return cls(optimizer, normalization=normalization)\n",
    "\n",
    "\n",
    "_NORMS = {\n",
    "    'l2': l2_norm,\n",
    "}\n",
    "\n",
    "# register this optimizer to the global custom objects when it is imported\n",
    "get_custom_objects().update({'NormalizedOptimizer': NormalizedOptimizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UXception(input_shape=(None, None, 3)):\n",
    "\n",
    "    backbone = Xception(input_shape=input_shape,weights='imagenet',include_top=False)\n",
    "    input = backbone.input\n",
    "    start_neurons = 16\n",
    "\n",
    "    conv4 = backbone.layers[121].output\n",
    "    conv4 = LeakyReLU(alpha=0.1)(conv4)\n",
    "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "    pool4 = Dropout(0.1)(pool4)\n",
    "    \n",
    "     # Middle\n",
    "    convm = Conv2D(start_neurons * 32, (3, 3), activation=None, padding=\"same\")(pool4)\n",
    "    convm = residual_block(convm,start_neurons * 32)\n",
    "    convm = residual_block(convm,start_neurons * 32)\n",
    "    convm = LeakyReLU(alpha=0.1)(convm)\n",
    "    \n",
    "    # 10 -> 20\n",
    "    deconv4 = Conv2DTranspose(start_neurons * 16, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
    "    uconv4 = concatenate([deconv4, conv4])\n",
    "    uconv4 = Dropout(0.1)(uconv4)\n",
    "    \n",
    "    uconv4 = Conv2D(start_neurons * 16, (3, 3), activation=None, padding=\"same\")(uconv4)\n",
    "    uconv4 = residual_block(uconv4,start_neurons * 16)\n",
    "    uconv4 = residual_block(uconv4,start_neurons * 16)\n",
    "    uconv4 = LeakyReLU(alpha=0.1)(uconv4)\n",
    "    \n",
    "    # 10 -> 20\n",
    "    deconv3 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n",
    "    conv3 = backbone.layers[31].output\n",
    "    uconv3 = concatenate([deconv3, conv3])    \n",
    "    uconv3 = Dropout(0.1)(uconv3)\n",
    "    \n",
    "    uconv3 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(uconv3)\n",
    "    uconv3 = residual_block(uconv3,start_neurons * 8)\n",
    "    uconv3 = residual_block(uconv3,start_neurons * 8)\n",
    "    uconv3 = LeakyReLU(alpha=0.1)(uconv3)\n",
    "\n",
    "    # 20 -> 40\n",
    "    deconv2 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n",
    "    conv2 = backbone.layers[21].output\n",
    "    conv2 = ZeroPadding2D(((1,0),(1,0)))(conv2)\n",
    "    uconv2 = concatenate([deconv2, conv2])\n",
    "        \n",
    "    uconv2 = Dropout(0.1)(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(uconv2)\n",
    "    uconv2 = residual_block(uconv2,start_neurons * 4)\n",
    "    uconv2 = residual_block(uconv2,start_neurons * 4)\n",
    "    uconv2 = LeakyReLU(alpha=0.1)(uconv2)\n",
    "    \n",
    "    # 40 -> 80\n",
    "    deconv1 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "    conv1 = backbone.layers[11].output\n",
    "    conv1 = ZeroPadding2D(((3,0),(3,0)))(conv1)\n",
    "    uconv1 = concatenate([deconv1, conv1])\n",
    "    \n",
    "    uconv1 = Dropout(0.1)(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(uconv1)\n",
    "    uconv1 = residual_block(uconv1,start_neurons * 2)\n",
    "    uconv1 = residual_block(uconv1,start_neurons * 2)\n",
    "    uconv1 = LeakyReLU(alpha=0.1)(uconv1)\n",
    "    \n",
    "    \n",
    "    # 80 -> 160\n",
    "    uconv0 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv1)   \n",
    "    uconv0 = Dropout(0.1)(uconv0)\n",
    "    uconv0 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv0)\n",
    "    uconv0 = residual_block(uconv0,start_neurons * 1)\n",
    "    uconv0 = residual_block(uconv0,start_neurons * 1)\n",
    "    uconv0 = LeakyReLU(alpha=0.1)(uconv0)\n",
    "    \n",
    "    uconv0 = Dropout(0.1/2)(uconv0)\n",
    "    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv0)    \n",
    "    \n",
    "    model = Model(input, output_layer)\n",
    "    model.name = 'u-xception'\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWA(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, filepath, swa_epoch):\n",
    "        super(SWA, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.swa_epoch = swa_epoch \n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.nb_epoch = self.params['epochs']\n",
    "        print('Stochastic weight averaging selected for last {} epochs.'\n",
    "              .format(self.nb_epoch - self.swa_epoch))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        if epoch == self.swa_epoch:\n",
    "            self.swa_weights = self.model.get_weights()\n",
    "            \n",
    "        elif epoch > self.swa_epoch:    \n",
    "            for i in range(len(self.swa_weights)):\n",
    "                self.swa_weights[i] = (self.swa_weights[i] * \n",
    "                    (epoch - self.swa_epoch) + self.model.get_weights()[i])/((epoch - self.swa_epoch)  + 1)  \n",
    "\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        self.model.set_weights(self.swa_weights)\n",
    "        print('Final model parameters set to stochastic weight average.')\n",
    "        self.model.save_weights(self.filepath)\n",
    "        print('Final stochastic averaged weights saved to file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model = UXception(input_shape=(TAR_SIZE,TAR_SIZE,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(0.005, momentum=0.9, nesterov=True)\n",
    "sgd = NormalizedOptimizer(sgd, normalization='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=bce_dice_loss, optimizer=sgd, metrics=[my_iou_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./models/XUnet256_swa_0.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_per_epoch = math.ceil(len(train_fold) / BATCH_SIZE / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic weight averaging selected for last -5 epochs.\n",
      "Epoch 1/30\n",
      "3369/3369 [==============================] - 1216s 361ms/step - loss: 0.1632 - my_iou_metric: 0.4454 - val_loss: 0.1805 - val_my_iou_metric: 0.4291\n",
      "\n",
      "Epoch 00001: val_my_iou_metric improved from -inf to 0.42914, saving model to ./models/XUnet384_0.model\n",
      "Epoch 2/30\n",
      "3369/3369 [==============================] - 1237s 367ms/step - loss: 0.1601 - my_iou_metric: 0.4459 - val_loss: 0.1703 - val_my_iou_metric: 0.4414\n",
      "\n",
      "Epoch 00002: val_my_iou_metric improved from 0.42914 to 0.44135, saving model to ./models/XUnet384_0.model\n",
      "Epoch 3/30\n",
      "3369/3369 [==============================] - 1240s 368ms/step - loss: 0.1539 - my_iou_metric: 0.4498 - val_loss: 0.1814 - val_my_iou_metric: 0.4421\n",
      "\n",
      "Epoch 00003: val_my_iou_metric improved from 0.44135 to 0.44211, saving model to ./models/XUnet384_0.model\n",
      "Epoch 4/30\n",
      "3369/3369 [==============================] - 1240s 368ms/step - loss: 0.1528 - my_iou_metric: 0.4556 - val_loss: 0.1774 - val_my_iou_metric: 0.4301\n",
      "\n",
      "Epoch 00004: val_my_iou_metric did not improve from 0.44211\n",
      "Epoch 5/30\n",
      "3369/3369 [==============================] - 1240s 368ms/step - loss: 0.1518 - my_iou_metric: 0.4557 - val_loss: 0.1700 - val_my_iou_metric: 0.4447\n",
      "\n",
      "Epoch 00005: val_my_iou_metric improved from 0.44211 to 0.44474, saving model to ./models/XUnet384_0.model\n",
      "Epoch 6/30\n",
      "3369/3369 [==============================] - 1240s 368ms/step - loss: 0.1503 - my_iou_metric: 0.4580 - val_loss: 0.1616 - val_my_iou_metric: 0.4577\n",
      "\n",
      "Epoch 00006: val_my_iou_metric improved from 0.44474 to 0.45771, saving model to ./models/XUnet384_0.model\n",
      "Epoch 7/30\n",
      "3369/3369 [==============================] - 1234s 366ms/step - loss: 0.1470 - my_iou_metric: 0.4649 - val_loss: 0.1720 - val_my_iou_metric: 0.4410\n",
      "\n",
      "Epoch 00007: val_my_iou_metric did not improve from 0.45771\n",
      "Epoch 8/30\n",
      "3369/3369 [==============================] - 1234s 366ms/step - loss: 0.1465 - my_iou_metric: 0.4623 - val_loss: 0.1671 - val_my_iou_metric: 0.4477\n",
      "\n",
      "Epoch 00008: val_my_iou_metric did not improve from 0.45771\n",
      "Epoch 9/30\n",
      "3369/3369 [==============================] - 1234s 366ms/step - loss: 0.1436 - my_iou_metric: 0.4662 - val_loss: 0.1627 - val_my_iou_metric: 0.4519\n",
      "\n",
      "Epoch 00009: val_my_iou_metric did not improve from 0.45771\n",
      "Epoch 10/30\n",
      "3369/3369 [==============================] - 1234s 366ms/step - loss: 0.1442 - my_iou_metric: 0.4702 - val_loss: 0.1821 - val_my_iou_metric: 0.4367\n",
      "\n",
      "Epoch 00010: val_my_iou_metric did not improve from 0.45771\n",
      "Epoch 11/30\n",
      "3369/3369 [==============================] - 1234s 366ms/step - loss: 0.1437 - my_iou_metric: 0.4667 - val_loss: 0.1641 - val_my_iou_metric: 0.4464\n",
      "\n",
      "Epoch 00011: val_my_iou_metric did not improve from 0.45771\n",
      "Epoch 12/30\n",
      "3369/3369 [==============================] - 1235s 366ms/step - loss: 0.1417 - my_iou_metric: 0.4688 - val_loss: 0.1638 - val_my_iou_metric: 0.4637\n",
      "\n",
      "Epoch 00012: val_my_iou_metric improved from 0.45771 to 0.46372, saving model to ./models/XUnet384_0.model\n",
      "Epoch 13/30\n",
      "3369/3369 [==============================] - 1235s 367ms/step - loss: 0.1407 - my_iou_metric: 0.4752 - val_loss: 0.1628 - val_my_iou_metric: 0.4662\n",
      "\n",
      "Epoch 00013: val_my_iou_metric improved from 0.46372 to 0.46617, saving model to ./models/XUnet384_0.model\n",
      "Epoch 14/30\n",
      "3369/3369 [==============================] - 1235s 367ms/step - loss: 0.1405 - my_iou_metric: 0.4749 - val_loss: 0.1616 - val_my_iou_metric: 0.4562\n",
      "\n",
      "Epoch 00014: val_my_iou_metric did not improve from 0.46617\n",
      "Epoch 15/30\n",
      "3369/3369 [==============================] - 1235s 367ms/step - loss: 0.1385 - my_iou_metric: 0.4740 - val_loss: 0.1600 - val_my_iou_metric: 0.4620\n",
      "\n",
      "Epoch 00015: val_my_iou_metric did not improve from 0.46617\n",
      "Epoch 16/30\n",
      "3369/3369 [==============================] - 1235s 367ms/step - loss: 0.1385 - my_iou_metric: 0.4814 - val_loss: 0.1635 - val_my_iou_metric: 0.4566\n",
      "\n",
      "Epoch 00016: val_my_iou_metric did not improve from 0.46617\n",
      "Epoch 17/30\n",
      "3369/3369 [==============================] - 1235s 367ms/step - loss: 0.1386 - my_iou_metric: 0.4779 - val_loss: 0.1708 - val_my_iou_metric: 0.4449\n",
      "\n",
      "Epoch 00017: val_my_iou_metric did not improve from 0.46617\n",
      "Epoch 18/30\n",
      "3369/3369 [==============================] - 1235s 366ms/step - loss: 0.1363 - my_iou_metric: 0.4781 - val_loss: 0.1590 - val_my_iou_metric: 0.4754\n",
      "\n",
      "Epoch 00018: val_my_iou_metric improved from 0.46617 to 0.47538, saving model to ./models/XUnet384_0.model\n",
      "Epoch 19/30\n",
      "3369/3369 [==============================] - 1235s 367ms/step - loss: 0.1344 - my_iou_metric: 0.4854 - val_loss: 0.1625 - val_my_iou_metric: 0.4573\n",
      "\n",
      "Epoch 00019: val_my_iou_metric did not improve from 0.47538\n",
      "Epoch 20/30\n",
      "3369/3369 [==============================] - 1235s 367ms/step - loss: 0.1359 - my_iou_metric: 0.4819 - val_loss: 0.1561 - val_my_iou_metric: 0.4701\n",
      "\n",
      "Epoch 00020: val_my_iou_metric did not improve from 0.47538\n",
      "Epoch 21/30\n",
      "3369/3369 [==============================] - 1235s 367ms/step - loss: 0.1330 - my_iou_metric: 0.4828 - val_loss: 0.1537 - val_my_iou_metric: 0.4603\n",
      "\n",
      "Epoch 00021: val_my_iou_metric did not improve from 0.47538\n",
      "Epoch 22/30\n",
      "3369/3369 [==============================] - 1235s 367ms/step - loss: 0.1323 - my_iou_metric: 0.4879 - val_loss: 0.1728 - val_my_iou_metric: 0.4402\n",
      "\n",
      "Epoch 00022: val_my_iou_metric did not improve from 0.47538\n",
      "Epoch 23/30\n",
      "3369/3369 [==============================] - 1235s 367ms/step - loss: 0.1337 - my_iou_metric: 0.4851 - val_loss: 0.1534 - val_my_iou_metric: 0.4705\n",
      "\n",
      "Epoch 00023: val_my_iou_metric did not improve from 0.47538\n",
      "Epoch 24/30\n",
      "3369/3369 [==============================] - 1235s 367ms/step - loss: 0.1315 - my_iou_metric: 0.4866 - val_loss: 0.1466 - val_my_iou_metric: 0.4820\n",
      "\n",
      "Epoch 00024: val_my_iou_metric improved from 0.47538 to 0.48195, saving model to ./models/XUnet384_0.model\n",
      "Epoch 25/30\n",
      "3369/3369 [==============================] - 1235s 367ms/step - loss: 0.1311 - my_iou_metric: 0.4934 - val_loss: 0.1665 - val_my_iou_metric: 0.4596\n",
      "\n",
      "Epoch 00025: val_my_iou_metric did not improve from 0.48195\n",
      "Epoch 26/30\n",
      "3369/3369 [==============================] - 1234s 366ms/step - loss: 0.1307 - my_iou_metric: 0.4892 - val_loss: 0.1665 - val_my_iou_metric: 0.4575\n",
      "\n",
      "Epoch 00026: val_my_iou_metric did not improve from 0.48195\n",
      "Epoch 27/30\n",
      "3369/3369 [==============================] - 1235s 367ms/step - loss: 0.1290 - my_iou_metric: 0.4914 - val_loss: 0.1550 - val_my_iou_metric: 0.4686\n",
      "\n",
      "Epoch 00027: val_my_iou_metric did not improve from 0.48195\n",
      "Epoch 28/30\n",
      "3369/3369 [==============================] - 1235s 367ms/step - loss: 0.1277 - my_iou_metric: 0.4969 - val_loss: 0.1615 - val_my_iou_metric: 0.4618\n",
      "\n",
      "Epoch 00028: val_my_iou_metric did not improve from 0.48195\n",
      "Epoch 29/30\n",
      "  50/3369 [..............................] - ETA: 20:05 - loss: 0.1154 - my_iou_metric: 0.4930"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-f55cb41688ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                     callbacks=snapshot.get_callbacks(),shuffle=True,verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "snapshot = SnapshotCallbackBuilder(nb_epochs=epochs,nb_snapshots=1,init_lr=1e-3, fold=fold)\n",
    "swa = SWA('./models/XUnet{}_swa_{}.model'.format(TAR_SIZE, fold), 35)\n",
    "history = model.fit_generator(train_gen, step_per_epoch,\n",
    "                    validation_data = (val_x, val_y),\n",
    "                    epochs=epochs,\n",
    "                    workers = 3,\n",
    "                    callbacks=snapshot.get_callbacks(),shuffle=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using swa weight model\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('using swa weight model')\n",
    "    model.load_weights('./models/XUnet{}_swa_{}.model'.format(TAR_SIZE, fold))\n",
    "except:\n",
    "    model.load_weights('./models/XUnet{}_{}.model'.format(TAR_SIZE, fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_result(model,x_test,img_size_target,batch_size): # predict both orginal and reflect x\n",
    "    x_test_reflect =  np.array([np.fliplr(x) for x in x_test])\n",
    "    preds_test1 = model.predict(x_test,batch_size=batch_size).reshape(-1, img_size_target, img_size_target)\n",
    "    preds_test2_refect = model.predict(x_test_reflect,batch_size=batch_size).reshape(-1, img_size_target, img_size_target)\n",
    "    preds_test2 = np.array([ np.fliplr(x) for x in preds_test2_refect] )\n",
    "    preds_avg = (preds_test1 +preds_test2)/2\n",
    "    return preds_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_fold = folds.loc[folds.fold == fold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_steps = math.ceil(len(oof_fold) / BATCH_SIZE)\n",
    "oof_gen = val_data_generator_sigmoid(oof_fold, masks, oof_steps, oof_steps, INPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_x, oof_y = next(oof_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2130, 384, 384, 1)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "preds_test1 = model.predict(oof_x,batch_size=batch_size)\n",
    "x_test_reflect =  np.array([np.fliplr(x) for x in oof_x])\n",
    "preds_test2_refect = model.predict(x_test_reflect,batch_size=batch_size)\n",
    "preds_test2 = np.array([ np.fliplr(x) for x in preds_test2_refect] )\n",
    "preds_avg = (preds_test1 +preds_test2)/2\n",
    "print(preds_avg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src: https://www.kaggle.com/aglotero/another-iou-metric\n",
    "def iou_metric(y_true_in, y_pred_in, print_table=False):\n",
    "    labels = y_true_in\n",
    "    y_pred = y_pred_in\n",
    "    \n",
    "    true_objects = 2\n",
    "    pred_objects = 2\n",
    "\n",
    "    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n",
    "\n",
    "    # Compute areas (needed for finding the union between all objects)\n",
    "    area_true = np.histogram(labels, bins = true_objects)[0]\n",
    "    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    # Compute union\n",
    "    union = area_true + area_pred - intersection\n",
    "\n",
    "    # Exclude background from the analysis\n",
    "    intersection = intersection[1:,1:]\n",
    "    union = union[1:,1:]\n",
    "    union[union == 0] = 1e-9\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    iou = intersection / union\n",
    "\n",
    "    # Precision helper function\n",
    "    def precision_at(threshold, iou):\n",
    "        matches = iou > threshold\n",
    "        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n",
    "        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
    "        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n",
    "        return tp, fp, fn\n",
    "\n",
    "    # Loop over IoU thresholds\n",
    "    prec = []\n",
    "    if print_table:\n",
    "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tp, fp, fn = precision_at(t, iou)\n",
    "        if (tp + fp + fn) > 0:\n",
    "            p = tp / (tp + fp + fn)\n",
    "        else:\n",
    "            p = 0\n",
    "        if print_table:\n",
    "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n",
    "        prec.append(p)\n",
    "    \n",
    "    if print_table:\n",
    "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
    "    return np.mean(prec)\n",
    "\n",
    "def iou_metric_batch(y_true_in, y_pred_in):\n",
    "    batch_size = y_true_in.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n",
    "        metric.append(value)\n",
    "    return np.mean(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544c17d3651b400eb3ab181e4a566442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Scoring for last model\n",
    "thresholds = np.linspace(0.4, 0.6, 21)\n",
    "ious = np.array([iou_metric_batch(oof_y, np.int32(preds_avg > threshold)) for threshold in tqdm_notebook(thresholds)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f5161d7f5c0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "threshold_best_index = np.argmax(ious) \n",
    "iou_best = ious[threshold_best_index]\n",
    "threshold_best = thresholds[threshold_best_index]\n",
    "\n",
    "plt.plot(thresholds, ious)\n",
    "plt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"IoU\")\n",
    "plt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del oof_x, oof_y,preds_avg\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../input/sample_submission_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002bd58.jpg</td>\n",
       "      <td>1 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00015efb6.jpg</td>\n",
       "      <td>1 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00023d5fc.jpg</td>\n",
       "      <td>1 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000367c13.jpg</td>\n",
       "      <td>1 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0008ca6e9.jpg</td>\n",
       "      <td>1 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ImageId EncodedPixels\n",
       "0  00002bd58.jpg           1 2\n",
       "1  00015efb6.jpg           1 2\n",
       "2  00023d5fc.jpg           1 2\n",
       "3  000367c13.jpg           1 2\n",
       "4  0008ca6e9.jpg           1 2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "preds_test = []\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "step_id = 0\n",
    "for idx, row in test_df.iterrows():\n",
    "    img_id = row.ImageId\n",
    "    imgpath = os.path.join('../input/test_v2/', '{0}'.format(img_id))\n",
    "\n",
    "    img = cv2.imread(imgpath, cv2.IMREAD_COLOR)\n",
    "    img = cv2.resize(img, INPUT_SHAPE)\n",
    "    inputs.append(img)\n",
    "inputs = np.asarray(inputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict orignal test\n",
      "predict flip left-right test\n",
      "predict flip up-down\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "print('predict orignal test')\n",
    "preds_test1 = model.predict(inputs,batch_size=batch_size)\n",
    "\n",
    "print('predict flip left-right test')\n",
    "x_test_reflect =  np.array([np.fliplr(x) for x in inputs])\n",
    "preds_test2_refect = model.predict(x_test_reflect,batch_size=batch_size)\n",
    "preds_test2 = np.array([ np.fliplr(x) for x in preds_test2_refect] )\n",
    "\n",
    "print('predict flip up-down')\n",
    "x_test_ud =  np.array([np.flipud(x) for x in inputs])\n",
    "preds_test3_refect = model.predict(x_test_ud,batch_size=batch_size)\n",
    "preds_test3 = np.array([ np.flipud(x) for x in preds_test3_refect] )\n",
    "\n",
    "preds_avg = (preds_test1 +preds_test2 + preds_test3) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15606, 384, 384, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_avg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_bool = (preds_avg > threshold_best ) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_encode(im):\n",
    "    '''\n",
    "    im: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = im.flatten(order = 'F')\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15606, 384, 384, 1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_bool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877354fa097946e2955da1aa94886a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15606), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm_notebook(range(preds_bool.shape[0])):\n",
    "    rle = ''\n",
    "    if(preds_bool[i].max() > 0):\n",
    "        #plt.imshow(preds_bool[i].squeeze())\n",
    "        pre = preds_bool[i].astype(np.float).squeeze()\n",
    "        ori_pred = ((cv2.resize(pre, (768,768)) > 0.5) * 1).astype(np.uint8)\n",
    "        rle = rle_encode(ori_pred)\n",
    "\n",
    "    test_df.iloc[i, 1] = rle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('../result/XUnet256_0.csv', index=False)\n",
    "one_result = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_detect = pd.read_csv('../result/ship_detection.csv')\n",
    "ship_detect.rename({'id':'ImageId'}, axis = 1, inplace = True)\n",
    "df_all = pd.merge(one_result, ship_detect, on='ImageId', how='left')\n",
    "df_all.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_mask(mask):\n",
    "    threshold = 0.5\n",
    "    threshold_obj = 0 #ignor predictions composed of \"threshold_obj\" pixels or less\n",
    "    labled,n_objs = ndimage.label(mask)\n",
    "    result = []\n",
    "    for i in range(n_objs):\n",
    "        obj = (labled == i + 1).astype(int)\n",
    "        if(obj.sum() > threshold_obj): result.append(obj)\n",
    "    return result\n",
    "\n",
    "def decode_mask(mask, shape=(768, 768)):\n",
    "    pixels = mask.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_result.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "def get_mask(encodepixel):\n",
    "    shape = (768,768)\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    masks = encodepixel\n",
    "    \n",
    "#     print(img_id, masks)\n",
    "    \n",
    "    if(type(masks.iloc[0]) == float): return img.reshape(shape)\n",
    "    for mask in masks:\n",
    "        s = mask.split()\n",
    "        for i in range(len(s)//2):\n",
    "            start = int(s[2*i]) - 1\n",
    "            length = int(s[2*i+1])\n",
    "            img[start:start+length] = 1\n",
    "    return img.reshape(shape).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1813b725e09b404994f17ab8f92d1377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_result = pd.DataFrame(columns=one_result.columns)\n",
    "cnt = 0\n",
    "for idx, row in tqdm_notebook(one_result.iterrows()):\n",
    "    if len(row.EncodedPixels) == 0:\n",
    "        new_result = new_result.append(row, ignore_index=True)\n",
    "    else:\n",
    "        mask = get_mask(one_result.loc[one_result.ImageId == row.ImageId, 'EncodedPixels'])\n",
    "        results = split_mask(mask)\n",
    "        \n",
    "        for msk in results:\n",
    "            new_result = new_result.append({'ImageId':row.ImageId, 'EncodedPixels': decode_mask(msk)}, ignore_index=True)\n",
    "#         cnt += 1\n",
    "#         if cnt > 10:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save result\n",
      "upload result\n",
      "cmd: kaggle competitions submit -c airbus-ship-detection -f ../result/XUnet384_split.csv.7z -m \"submit\"\n"
     ]
    }
   ],
   "source": [
    "kaggle_util.save_result(new_result, '../result/XUnet384_split.csv', competition = 'airbus-ship-detection', send = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
